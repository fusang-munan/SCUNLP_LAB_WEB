---
title: "ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming"
authors:
- Xinwei Yang
date: '2025-05-22'
publication_types:
- "paper-conference"
publication: "*ACL 2025*"
tags:
- CCF-A
abstract: "While recent research increasingly emphasizes the value of human-LLM collaboration in competitive programming and proposes numerous empirical methods, a comprehensive understanding remains elusive due to the fragmented nature of existing studies and their use of diverse, application-specific human feedback. Thus, our work serves a three-fold purpose: First, we present the first taxonomy of human feedback consolidating the entire programming process, which promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a novel programming dataset specifically designed for human-LLM collaboration, meticulously annotated to enable large-scale simulated human feedback and facilitate costeffective real human interaction studies. Third, we introduce ELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM competitive programming. With ELABORATION, we pinpoint strengthes and weaknesses of existing methods, thereby setting the foundation for future improvement."
url_pdf: "https://arxiv.org/abs/2505.16667"
---
